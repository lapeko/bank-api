name: Deploy and prepare infrastructure

on:
  push:
    branches: [main]
    paths: 
    - 'infra/terraform/**'
    - '.github/workflows/infra.yaml'


concurrency:
  group: infra-stage
  cancel-in-progress: true

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    env:
      ReleaseName: "bank-api"
      TerraPath: "./infra/terraform"
      HelmPath: "./infra/k8s"
    steps:
      - name: Checkout repository 
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          aws-region: eu-central-1
          role-to-assume: arn:aws:iam::539247467338:role/my-gha-role

      - name: Ensure S3 bucket exists
        run: |
          BUCKET_NAME="${{ env.ReleaseName }}-tfstate"
          if aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "S3 bucket $BUCKET_NAME already exists"
          else
            echo "Creating S3 bucket $BUCKET_NAME"
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region eu-central-1 \
              --create-bucket-configuration LocationConstraint=eu-central-1
            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption \
              --bucket "$BUCKET_NAME" \
              --server-side-encryption-configuration '{
                "Rules": [{
                  "ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}
                }]
              }'
          fi

      - name: Ensure DynamoDB table exists
        run: |
          TABLE_NAME="${{ env.ReleaseName }}-tf-locks"
          if aws dynamodb describe-table --table-name "$TABLE_NAME" 2>/dev/null; then
            echo "DynamoDB table $TABLE_NAME already exists"
          else
            echo "Creating DynamoDB table $TABLE_NAME"
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5
          fi

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init Stage1
        run: terraform -chdir=${{ env.TerraPath }}/stage1 init -input=false

      - name: Terraform Validate Stage1
        run: terraform -chdir=${{ env.TerraPath }}/stage1 validate

      - name: Terraform Plan Stage1
        run: terraform -chdir=${{ env.TerraPath }}/stage1 plan -input=false

      - name: Terraform Apply Stage1
        run: terraform -chdir=${{ env.TerraPath }}/stage1 apply -input=false -auto-approve

      - name: Export Terraform outputs from Stage1
        run: |
          cd infra/terraform/stage1
          echo "VPC_ID=$(terraform output -raw vpc_id)" >> $GITHUB_ENV
          echo "ALB_ROLE_ARN=$(terraform output -raw alb_controller_role_arn)" >> $GITHUB_ENV

      - name: Install Helm
        uses: azure/setup-helm@v4.3.0

      - name: Wait EKS becomes active
        run: aws eks wait cluster-active --region eu-central-1 --name ${{ env.ReleaseName }}-eks

      - name: Grant EKS access to my IAM user
        run: |
          set -euo pipefail
          CLUSTER="${{ env.ReleaseName }}-eks"
          PRINCIPAL="arn:aws:iam::539247467338:user/vitali"

          aws eks create-access-entry \
            --cluster-name "$CLUSTER" \
            --principal-arn "$PRINCIPAL" \
            --type STANDARD \
          >/dev/null 2>&1 || echo "access entry exists"

          aws eks associate-access-policy \
            --cluster-name "$CLUSTER" \
            --principal-arn "$PRINCIPAL" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
          >/dev/null 2>&1 || echo "policy exists"

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig \
            --region eu-central-1 \
            --name ${{ env.ReleaseName }}-eks

      - name: Install AWS Load Balancer Controller
        run: |
          helm repo add eks https://aws.github.io/eks-charts --force-update
          helm repo update
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system --create-namespace --wait --atomic \
            --set clusterName=${{ env.ReleaseName }}-eks \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=${ALB_ROLE_ARN} \
            --set region=eu-central-1 \
            --set vpcId=${VPC_ID}
          kubectl -n kube-system rollout status deploy/aws-load-balancer-controller --timeout=5m

      - name: Install external-secrets
        run: |
          helm repo add external-secrets https://charts.external-secrets.io --force-update
          helm repo update
          helm upgrade --install external-secrets external-secrets/external-secrets \
            -n external-secrets --create-namespace --wait --atomic \
            --set installCRDs=true
          kubectl wait --for=condition=Established crd/externalsecrets.external-secrets.io --timeout=120s
          kubectl wait --for=condition=Established crd/clustersecretstores.external-secrets.io --timeout=120s

      - name: Deploy app chart
        run: |
          helm upgrade --install ${{ env.ReleaseName }} ${{ env.HelmPath }} \
            -n ${{ env.ReleaseName }} --create-namespace \
            --set deploy.image.repo=nginx \
            --set deploy.image.tag=1.27-alpine \
            --set deploy.port=80 \
            --set service.port=80 \
            --set ingress.healthcheckPath=/

      - name: Wait for Ingress hostname
        run: |
          set -euo pipefail
          NS="${{ env.ReleaseName }}"
          ING="${{ env.ReleaseName }}-ingress"
          for i in {1..60}; do
            H=$(kubectl -n "$NS" get ing "$ING" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || true)
            if [ -n "$H" ]; then echo "Ingress hostname: $H"; exit 0; fi
            echo "Waiting for Ingress hostname... ($i/60)"; sleep 10
          done
          echo "ERROR: Ingress did not get a hostname in time"; exit 1

      - name: Debug if ingress missing
        if: failure()
        run: |
          kubectl -n ${{ env.ReleaseName }} describe ingress ${{ env.ReleaseName }}-ingress || true
          kubectl -n kube-system logs -l app.kubernetes.io/name=aws-load-balancer-controller --tail=200 || true

      - name: Terraform Init Stage2
        run: terraform -chdir=${{ env.TerraPath }}/stage2 init -input=false

      - name: Terraform Validate Stage2
        run: terraform -chdir=${{ env.TerraPath }}/stage2 validate

      - name: Terraform Plan Stage2
        run: terraform -chdir=${{ env.TerraPath }}/stage2 plan -input=false

      - name: Terraform Apply Stage2
        run: terraform -chdir=${{ env.TerraPath }}/stage2 apply -input=false -auto-approve